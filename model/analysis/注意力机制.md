## 目录
  
- [1. External Attention](#1-external-attention)

- [2. Self Attention](#2-self-attention)

- [3. Squeeze-and-Excitation(SE) Attention](#3-squeeze-and-excitationse-attention)

- [4. Selective Kernel(SK) Attention](#4-selective-kernelsk-attention)

- [5. CBAM Attention](#5-cbam-attention)

- [6. BAM Attention](#6-bam-attention)

- [7. ECA Attention](#7-eca-attention)

- [8. DANet Attention](#8-danet-attention)

- [9. Pyramid Split Attention(PSA)](#9-pyramid-split-attentionpsa)

- [10. Efficient Multi-Head Self-Attention(EMSA)](#10-efficient-multi-head-self-attentionemsa)

- [11. Tumor Attention(TA)](#11-tumor-attentionta)

- [12. CAAGP Attention](#12-caagp-attention)

- [13. Axial Attention](#13-axial-attention)

- [14. MaxViT](#14-maxvit)

- [15. PA Attention](#15-pa-attention)

- [16. CSE Attention](#16-cse-attention)
- [17. 先通道后空间 Attention](#17-先通道后空间-attention)
- [18. GC Attention](#18-gc-attention)
- [19. DCAC](#19-dcac)
- [【写在最后】](#写在最后)



## 1. External Attention

### 1.1. 引用

Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks.---arXiv 2021.05.05

论文地址：[https://arxiv.org/abs/2105.02358](https://arxiv.org/abs/2105.02358)

### 1.2. 模型结构
<img width="529" alt="image" src="https://user-images.githubusercontent.com/63939745/183250672-b0466516-3de6-47e6-95bf-f0264d7fe6b1.png">

![](./img/External_Attention.png)

### 1.3. 简介

这是五月份在arXiv上的一篇文章，主要解决的Self-Attention(SA)的两个痛点问题：（1）O(n^2)的计算复杂度；(2)SA是在同一个样本上根据不同位置计算Attention，忽略了不同样本之间的联系。因此，本文采用了两个串联的MLP结构作为memory units，使得计算复杂度降低到了O(n)；此外，这两个memory units是基于全部的训练数据学习的，因此也隐式的考虑了不同样本之间的联系。

### 1.4. 使用方法

```python
from attention.ExternalAttention import ExternalAttention
import torch


input=torch.randn(50,49,512)
ea = ExternalAttention(d_model=512,S=8)
output=ea(input)
print(output.shape)
```





## 2. Self Attention

### 2.1. 引用

Attention Is All You Need---NeurIPS2017

论文地址：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

### 2.2. 模型结构

![](./img/SA.png)

### 2.3. 简介

这是Google在NeurIPS2017发表的一篇文章，在CV、NLP、多模态等各个领域都有很大的影响力，目前引用量已经2.2w+。Transformer中提出的Self-Attention是Attention的一种，用于计算特征中不同位置之间的权重，从而达到更新特征的效果。首先将input feature通过FC映射成Q、K、V三个特征，然后将Q和K进行点乘的得到attention map，在将attention map与V做点乘得到加权后的特征。最后通过FC进行特征的映射，得到一个新的特征。（关于Transformer和Self-Attention目前网上有许多非常好的讲解，这里就不做详细的介绍了）

### 2.4. 使用方法

```python
from attention.SelfAttention import ScaledDotProductAttention
import torch

input=torch.randn(50,49,512)
sa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)
output=sa(input,input,input)
print(output.shape)
```





## 3. Squeeze-and-Excitation(SE) Attention

### 3.1. 引用

Squeeze-and-Excitation Networks---CVPR2018

论文地址：[https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)

### 3.2. 模型结构
<img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/183251779-f80395cd-78ec-4b46-9318-054644155b26.png">

![](./img/SE.png)

### 3.3. 简介

这是CVPR2018的一篇文章，同样非常具有影响力，目前引用量7k+。本文是做通道注意力的，因其简单的结构和有效性，将通道注意力掀起了一波小高潮。大道至简，这篇文章的思想可以说非常简单，首先将spatial维度进行AdaptiveAvgPool，然后通过两个FC学习到通道注意力，并用Sigmoid进行归一化得到Channel Attention Map,最后将Channel Attention Map与原特征相乘，就得到了加权后的特征。

### 3.4. 使用方法

```python
from attention.SEAttention import SEAttention
import torch

input=torch.randn(50,512,7,7)
se = SEAttention(channel=512,reduction=8)
output=se(input)
print(output.shape)
```



 

## 4. Selective Kernel(SK) Attention

### 4.1. 引用

Selective Kernel Networks---CVPR2019

论文地址：[https://arxiv.org/pdf/1903.06586.pdf](https://arxiv.org/pdf/1903.06586.pdf)

### 4.2. 模型结构

<img width="576" alt="image" src="https://user-images.githubusercontent.com/63939745/183274772-86b011cc-b1c2-45e5-a9af-e49fa24723c9.png">

### 4.3. 简介

这是CVPR2019的一篇文章，致敬了SENet的思想。在传统的CNN中每一个卷积层都是用相同大小的卷积核，限制了模型的表达能力；而Inception这种“更宽”的模型结构也验证了，用多个不同的卷积核进行学习确实可以提升模型的表达能力。作者借鉴了SENet的思想，通过动态计算每个卷积核得到通道的权重，动态的将各个卷积核的结果进行融合。

个人认为，之所以所这篇文章也能够称之为lightweight，是因为对不同kernel的特征进行通道注意力的时候是参数共享的（i.e. 因为在做Attention之前，首先将特征进行了融合，所以不同卷积核的结果共享一个SE模块的参数）。

本文的方法分为三个部分：Split,Fuse,Select。Split就是一个multi-branch的操作，用不同的卷积核进行卷积得到不同的特征；Fuse部分就是用SE的结构获取通道注意力的矩阵(N个卷积核就可以得到N个注意力矩阵，这步操作对所有的特征参数共享)，这样就可以得到不同kernel经过SE之后的特征；Select操作就是将这几个特征进行相加。

### 4.4. 使用方法

```python
from attention.SKAttention import SKAttention
import torch

input=torch.randn(50,512,7,7)
se = SKAttention(channel=512,reduction=8)
output=se(input)
print(output.shape)
```



 

## 5. CBAM Attention

### 5.1. 引用

CBAM: Convolutional Block Attention Module---ECCV2018

论文地址：[https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)

### 5.2. 模型结构

<img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/183274796-88a84b1c-f933-4c15-aef2-743a3c83e035.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/183274816-86fa4d7f-c54d-44a6-956c-7f8470a461fc.png">


### 5.3. 简介

这是ECCV2018的一篇论文，这篇文章同时使用了Channel Attention和Spatial Attention，将两者进行了串联（文章也做了并联和两种串联方式的消融实验）。

Channel Attention方面，大致结构还是和SE相似，不过作者提出AvgPool和MaxPool有不同的表示效果，所以作者对原来的特征在Spatial维度分别进行了AvgPool和MaxPool，然后用SE的结构提取channel attention，注意这里是参数共享的，然后将两个特征相加后做归一化，就得到了注意力矩阵。

Spatial Attention和Channel Attention类似，先在channel维度进行两种pool后，将两个特征进行拼接，然后用7x7的卷积来提取Spatial Attention（之所以用7x7是因为提取的是空间注意力，所以用的卷积核必须足够大）。然后做一次归一化，就得到了空间的注意力矩阵。

### 5.4. 使用方法

```python
from attention.CBAM import CBAMBlock
import torch

input=torch.randn(50,512,7,7)
kernel_size=input.shape[2]
cbam = CBAMBlock(channel=512,reduction=16,kernel_size=kernel_size)
output=cbam(input)
print(output.shape)
```



 

## 6. BAM Attention

### 6.1. 引用

BAM: Bottleneck Attention Module---BMCV2018

论文地址：[https://arxiv.org/pdf/1807.06514.pdf](https://arxiv.org/pdf/1807.06514.pdf)

### 6.2. 模型结构

<img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/183274834-b6b481e4-64ed-4d81-8d8e-e289485a2e74.png">


### 6.3. 简介

这是CBAM同作者同时期的工作，工作与CBAM非常相似，也是双重Attention，不同的是CBAM是将两个attention的结果串联；而BAM是直接将两个attention矩阵进行相加。

Channel Attention方面，与SE的结构基本一样。Spatial Attention方面，还是在通道维度进行pool，然后用了两次3x3的空洞卷积，最后将用一次1x1的卷积得到Spatial Attention的矩阵。

最后Channel Attention和Spatial Attention矩阵进行相加（这里用到了广播机制），并进行归一化，这样一来，就得到了空间和通道结合的attention矩阵。

### 6.4.使用方法

```python
from attention.BAM import BAMBlock
import torch

input=torch.randn(50,512,7,7)
bam = BAMBlock(channel=512,reduction=16,dia_val=2)
output=bam(input)
print(output.shape)
```





## 7. ECA Attention

### 7.1. 引用

ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks---CVPR2020

论文地址：[https://arxiv.org/pdf/1910.03151.pdf](https://arxiv.org/pdf/1910.03151.pdf)

### 7.2. 模型结构
<img width="472" alt="image" src="https://user-images.githubusercontent.com/63939745/183274879-928819d7-b740-49e6-9fda-c0aecbaf1174.png">


### 7.3. 简介

这是CVPR2020的一篇文章。

如上图所示，SE实现通道注意力是使用两个全连接层，而ECA是需要一个的卷积。作者这么做的原因一方面是认为计算所有通道两两之间的注意力是没有必要的，另一方面是用两个全连接层确实引入了太多的参数和计算量。

因此作者进行了AvgPool之后，只是使用了一个感受野为k的一维卷积（相当于只计算与相邻k个通道的注意力），这样做就大大的减少的参数和计算量。(i.e.相当于SE是一个global的注意力，而ECA是一个local的注意力)。

### 7.4. 使用方法：

```python
from attention.ECAAttention import ECAAttention
import torch

input=torch.randn(50,512,7,7)
eca = ECAAttention(kernel_size=3)
output=eca(input)
print(output.shape)
```



 

## 8. DANet Attention

### 8.1. 引用

Dual Attention Network for Scene Segmentation---CVPR2019

论文地址：[https://arxiv.org/pdf/1809.02983.pdf](https://arxiv.org/pdf/1809.02983.pdf)

### 8.2. 模型结构

<img width="400" alt="image" src="https://user-images.githubusercontent.com/63939745/183274906-aae5cd47-ad6a-412c-930f-c540e5478701.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/183274916-74a68a81-43a6-49f1-ae42-b2b25b602390.png">



### 8.3. 简介

这是CVPR2019的文章，思想上非常简单，就是将self-attention用到场景分割的任务中，不同的是self-attention是关注每个position之间的注意力，而本文将self-attention做了一个拓展，还做了一个通道注意力的分支，操作上和self-attention一样，不同的通道attention中把生成Q，K，V的三个Linear去掉了。最后将两个attention之后的特征进行element-wise sum。

### 8.4. 使用方法

```python
from attention.DANet import DAModule
import torch

input=torch.randn(50,512,7,7)
danet=DAModule(d_model=512,kernel_size=3,H=7,W=7)
print(danet(input).shape)
```



 

## 9. Pyramid Split Attention(PSA)

### 9.1. 引用

EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network---arXiv 2021.05.30

论文地址：[https://arxiv.org/pdf/2105.14447.pdf](https://arxiv.org/pdf/2105.14447.pdf)

### 9.2. 模型结构

<img width="325" alt="image" src="https://user-images.githubusercontent.com/63939745/183274932-d11832c4-02ec-4de9-9154-bca0d5a66d50.png"><img width="325" alt="image" src="https://user-images.githubusercontent.com/63939745/183274937-3c293fc5-d9fb-468c-bf8f-7f348d6fe744.png"><img width="350" alt="image" src="https://user-images.githubusercontent.com/63939745/183274948-7bf0a744-6214-4e18-b0b5-27bc11332b32.png">


### 9.3. 简介

这是深大5月30日在arXiv上上传的一篇文章，本文的目的是如何获取并探索不同尺度的空间信息来丰富特征空间。网络结构相对来说也比较简单，主要分成四步，第一步，将原来的feature根据通道分成n组然后对不同的组进行不同尺度的卷积，得到新的特征W1；第二步，用SE在原来的特征上进行SE，从而获得不同的Attention Map；第三步，对不同组进行SOFTMAX；第四步，将获得attention与原来的特征W1相乘。

### 9.4. 使用方法

```python
from attention.PSA import PSA
import torch

input=torch.randn(50,512,7,7)
psa = PSA(channel=512,reduction=8)
output=psa(input)
print(output.shape)
```



 

## 10. Efficient Multi-Head Self-Attention(EMSA)

### 10.1. 引用

ResT: An Efficient Transformer for Visual Recognition---arXiv 2021.05.28

论文地址：[https://arxiv.org/abs/2105.13677](https://arxiv.org/abs/2105.13677)

### 10.2. 模型结构

<img width="400" alt="image" src="https://user-images.githubusercontent.com/63939745/183274733-ab0b3300-1c12-4db9-a019-d593e0f1278c.png">

### 10.3. 简介

这是南大5月28日在arXiv上上传的一篇文章。本文解决的主要是SA的两个痛点问题：（1）Self-Attention的计算复杂度和n呈平方关系；（2）每个head只有q,k,v的部分信息，如果q,k,v的维度太小，那么就会导致获取不到连续的信息，从而导致性能损失。这篇文章给出的思路也非常简单，在SA中，在FC之前，用了一个卷积来降低了空间的维度，从而得到空间维度上更小的K和V。

### 10.4. 使用方法

```python
from attention.EMSA import EMSA
import torch
from torch import nn
from torch.nn import functional as F

input=torch.randn(50,64,512)
emsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)
output=emsa(input,input,input)
print(output.shape)
```


## 11. Tumor Attention(TA)

### 11.1. 引用
Tumor attention networks: Better feature selection, better tumor segmentation--- Published: March 2021  Neural Networks

论文地址：https://www.sciencedirect.com/science/article/abs/pii/S0893608021000861
### 11.2. 模型结构
<img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/184301577-2ec0fc5e-9e0c-495e-a868-868047485068.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/184305623-c60d2499-ba6a-4e17-9a56-c4b4f97d2d90.png">
### 11.3. 简介
不同于之前提出的注意模块，大多数只探索通道注意或空间注意，TA-Net注意力模块旨在沿着两个主要维度强调肝脏肿瘤的有意义的特征选择:通道和空间。从这个角度出发，在设计轻量级注意模块时，进一步增强了特征注意在两个维度上的有效性。此外， attention模块全局利用编码器和解码器路径中的所有浅高分辨率特征图和深高语义特征表示。相关博客：https://blog.csdn.net/weixin_49627776/article/details/123238799

## 12. CAAGP Attention

### 12.1. 引用
CAAGP: Rethinking channel attention with adaptive global pooling for liver tumor segmentation--- 2021 Elsevier Ltd.

论文地址：https://doi.org/10.1016/j.compbiomed.2021.104875 
### 12.2. 模型结构
<img width="900" alt="image" src="https://user-images.githubusercontent.com/63939745/184334799-8c44b8db-81f7-4f4b-a864-43096a4a7e81.png">
<img width="500" hight='600' alt="image" src="https://user-images.githubusercontent.com/63939745/184334854-e0d0db6f-8433-4b84-9aef-9feeb14df16a.png">

### 12.3. 简介
分别在两个方向做attention，宽度和高度上分别attention和融合，作者用于肝肿瘤分割上面，特别是对小肿瘤。能有效地吸收空间信息，将计算复杂度从二次降低到线性。图一中CBAM和BAM已经在前面记录过，没见代码在哪里

## 13. Axial Attention

### 13.1. 引用
Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation ---google

论文地址：[https://doi.org/10.1016/j.compbiomed.2021.104875](https://arxiv.org/pdf/2003.07853.pdf) 
### 13.2. 模型结构
<img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/184365164-f6346f82-b31a-4bc9-84f1-96a44b7e26e9.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/184366080-492cecb2-2d11-47e4-8a1c-458456d66e6a.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/184365964-bdb8f5ee-00f6-44d3-b8d6-4263b7ded290.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/184365736-934deba5-fc7c-4350-8f3d-7e133ca5f007.png">



### 13.3. 简介
分别在两个轴做attention，简称轴注意力。自我注意被用来通过非局部的相互作用来增强CNN。通过将注意力限制在局部区域来堆叠自我注意层来获得完全注意网络是可能的。其核心思想是将二维注意力依次分解为沿高度轴和宽度轴方向的两个一维注意力。降低了计算复杂度，并允许在更大甚至全局区域内进行注意。并提出了一种位置敏感型自我注意设计。两者结合在一起产生了位置敏感的轴向注意力层。

## 14. MaxViT

### 14.1. 引用
MaxViT: Multi-Axis Vision Transformer--

论文地址：[https://doi.org/10.1016/j.compbiomed.2021.104875](https://arxiv.org/pdf/2003.07853.pdf) ]
### 14.2. 模型结构
<img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/184369492-0821ee1e-8ce4-4ea3-9075-008c552ae59e.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/184369555-ddc3f71f-1788-4f8f-b7ce-78f3deeb0967.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/184370127-96cdfd37-317a-46c1-83e3-a3597980504e.png"><img width="394" alt="image" src="https://user-images.githubusercontent.com/63939745/184370262-41d3aa91-9d59-4512-9411-c3eb1b2db04c.png">


### 14.3. 简介
自注意力的机制对于图像大小方面缺乏可扩展性，限制了它们在视觉主干中的应用。本文提出了一种高效的可拓展的全局注意，该模型包括两个方面：阻塞的局部注意和拓展的全局注意。作者通过将该注意模型与卷积有效结合，并简单的将这些模块堆叠，形成了了一个分层的视觉主干网络MaxVit.与局部和窗口注意力不同，Max-SA通过提出一个全局感受野，获得了更强的模型容量。因为Max-SA具有线性复杂度，可以在网络中的任何曾作为一般的独立注意模块，即使是在高分辨率的早期。注意机制分解为两种稀疏形式：窗口注意和网格注意


## 15. PA Attention
### 15.1. 引用
PA-ResSeg: A Phase Attention Residual Network for Liver Tumor Segmentation from Multi-phase CT Images
地址：https://arxiv.org/pdf/2103.00274.pdf 
### 15.2. 模型结构
<img width="884" alt="image" src="https://user-images.githubusercontent.com/63939745/186047299-5ff6b155-e074-412a-86b4-b1575742d1f0.png">

### 15.3. 简介
使用两个相进行的多模态融合

## 16. CSE Attention
### 16.1. 引用
TA-Net: Triple attention network for medical image segmentation
地址：[https://arxiv.org/pdf/2103.00274.pdf ](https://sciencedirect.53yu.com/science/article/pii/S0010482521006302)
### 16.2. 模型结构
<img width="500" alt="sig  1  Owerall sachiteeture of ebe propsed Triele Aitenica Netwoek Se medical image semnentation" src="https://user-images.githubusercontent.com/63939745/188401558-5b13be71-9916-4d7a-90c3-21d1aa4928b5.png"><img width="500" alt="Softmax" src="https://user-images.githubusercontent.com/63939745/188402011-654b773e-b76a-4779-a9f7-3472f6a6abfc.png">
<img width="500" alt="ReLU" src="https://user-images.githubusercontent.com/63939745/188401600-0f248834-7866-4c19-a5c7-29eafd1c2f34.png">

### 16.3. 简介
作者说其他论文只使用两个注意力，得不到有效的表达。从通道域、空间域和特征内部域做注意力，实际上就是在编码器上通道注意力与selfattention做融合attention，解码器上与skip进行空间attention

## 17. 先通道后空间 Attention
### 17.1. 引用
Attention based multi-scale parallel network for polyp segmentation杂志（）
### 17.2. 模型结构
<img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/188415823-f3c45fc7-e984-4206-a547-d183160c8714.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/188464353-6a8befd4-9cea-4e03-945e-968d5ab1a507.png">


### 17.3. 简介
PSA首先进入通道分支，在重新分配权重值后，然后发送到空间分支，重新分配空间维度中的权重值。第二个图的RCF模块为什么这么做啊，reverse一下

## 18. GC Attention
### 18.1. 引用
GC-Net: Global context network for medical image segmentation杂志（Computer Methods and Programs in Biomedicine）
### 18.2. 模型结构
<img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/188524445-fdb6ed0a-b898-4ed3-aafe-f96dcccea557.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/188524285-a4e84af7-fbef-4b26-a489-a9f9171079c5.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/188524484-3e8d7e06-01c7-4c87-ad69-47c240fc4c1b.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/198004221-aec9ec1d-0c09-4798-87ae-8ab083bbdadc.png">

### 18.3. 简介
作者说一般网络结构忽略了全局上下文的信息，图2 中没有说为什么使用L2norm，只说为了产生全局的特征，损失函数也说的很模糊。

## 19. DCAC
### 19.1. 引用
DCACNet: Dual context aggregation and attention-guided cross deconvolution network for medical image segmentation
### 19.2. 模型结构
<img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/188525312-a5952619-ed64-413c-9360-2dcdfa45e689.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/188525357-0a936bdf-6edc-4be7-990b-05e75b836b5a.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/63939745/188525403-abaa33c0-a7a9-411e-a4a7-6e8cb2a90478.png"><img width="451" alt="image" src="https://user-images.githubusercontent.com/63939745/188525429-7673c73c-7f64-47b8-a6d7-ac0c141775ac.png"><img width="449" alt="image" src="https://user-images.githubusercontent.com/63939745/188525449-1dc8d02b-fd96-4797-a140-520f9dbc23ac.png">


### 19.3. 简介
使用了非常多的信息间的融合，可以说是非常的dense了，注意引导的交叉反褶积解码器网络

## 【写在最后】

目前该项目整理的Attention的工作确实还不够全面，后面随着阅读量的提高，会不断对本项目进行完善，欢迎大家star支持。若在文章中有表述不恰、代码实现有误的地方，欢迎大家指出~
